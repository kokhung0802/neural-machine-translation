{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bd60717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers import Layer, Embedding, GRU, Dense, AdditiveAttention\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277723c",
   "metadata": {},
   "source": [
    "**Shape Checker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b96b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker:\n",
    "    def __init__(self):\n",
    "        self.shapes = {}\n",
    "        \n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        # check whether it is a string\n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "            \n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "        \n",
    "        if rank != len(names):\n",
    "            raise ValueError(f'Rank mismatch:\\n'\n",
    "                             f'    found {rank}: {shape.numpy()}\\n'\n",
    "                             f'    expected {len(names)}: {names}\\n')\n",
    "        \n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "            \n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6bee6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2],[3,4],[5,6]]\n",
    "tf.rank(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc15de",
   "metadata": {},
   "source": [
    "**Download Dataset**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f40eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "578abdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "    \n",
    "    # lines[0] = 'Go.\\tVe.'\n",
    "    lines = text.splitlines()\n",
    "    # pairs[0] = ['Go.', 'Ve.']\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp in pairs]\n",
    "    targ = [targ for targ, inp in pairs]\n",
    "\n",
    "    return targ, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f5af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targ: english\n",
    "# inp: spanish\n",
    "targ, inp = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85d93892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
     ]
    }
   ],
   "source": [
    "print(inp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "701c6172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "print(targ[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3100d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tf.data.Dataset of strings that shuffles and batches them efficiently\n",
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96ee99c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'No estamos haciendo nada.' b'Est\\xc3\\xa1s gordo.'\n",
      " b'Tom triplic\\xc3\\xb3 su inversi\\xc3\\xb3n en seis meses.'\n",
      " b'No nos gusta la lluvia.' b'Hoy es once de octubre.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b\"We're doing nothing.\" b\"You're gross!\"\n",
      " b'Tom tripled his investment in six months.' b\"We don't like the rain.\"\n",
      " b'Today is October 11th.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784e801",
   "metadata": {},
   "source": [
    "**Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e97c0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Standardization\n",
    "# Unicode normalization - split accented characters and replace compatibility characters with their ASCII equivalents\n",
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e73e2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love apple#!\n",
      "[START] i love apple ! [END]\n"
     ]
    }
   ],
   "source": [
    "# demo\n",
    "example_text = tf.constant('i love apple#!')\n",
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57971b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) Text Vectoriaztion (encoding)\n",
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cceb513a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spanish TextVectorization layer\n",
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b08ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English TextVectorization layer\n",
    "output_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "236ce0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love apple#!\n",
      "tf.Tensor([  2   6 155 800 119   3], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# demo \n",
    "# these layers can convert a batch of strings into a batch of token IDs\n",
    "print(example_text.numpy().decode())\n",
    "example_tokens = output_text_processor(example_text)\n",
    "print(example_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9885aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[   2,    9,  172,  246,   69,    4,    3,    0,    0,    0],\n",
       "       [   2,   76, 1720,    4,    3,    0,    0,    0,    0,    0],\n",
       "       [   2,   10,    1,   25,    1,   14,  405,  796,    4,    3]],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4533fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] no estamos haciendo nada . [END]      '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_vocabulary method can be used to convert token IDs back to text\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03180360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd66c4",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a57f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6116220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embedding = Embedding(self.input_vocab_size, embedding_dim)\n",
    "        self.gru = GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "        \n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "        \n",
    "        # output shape: (batch, s, enc_units)\n",
    "        # state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "        \n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6c86c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (32,)\n",
      "Input batch tokens, shape (batch, s): (32, 13)\n",
      "Encoder output, shape (batch, s, units): (32, 13, 1024)\n",
      "Encoder state, shape (batch, units): (32, 1024)\n"
     ]
    }
   ],
   "source": [
    "# demo\n",
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f74d8",
   "metadata": {},
   "source": [
    "**Attention Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5e2e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = Dense(units, use_bias=False)\n",
    "        self.W2 = Dense(units, use_bias=False)\n",
    "        self.attention = AdditiveAttention()\n",
    "        \n",
    "    def call(self, query, value, mask):\n",
    "        def call(self, query, value, mask):\n",
    "            shape_checker = ShapeChecker()\n",
    "            shape_checker(query, ('batch', 't', 'query_units'))\n",
    "            shape_checker(value, ('batch', 's', 'value_units'))\n",
    "            shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "            # From Eqn. (4), `W1@ht`.\n",
    "            w1_query = self.W1(query)\n",
    "            shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "            # From Eqn. (4), `W2@hs`.\n",
    "            w2_key = self.W2(value)\n",
    "            shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "            query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "            value_mask = mask\n",
    "\n",
    "            context_vector, attention_weights = self.attention(\n",
    "                inputs = [w1_query, value, w2_key],\n",
    "                mask=[query_mask, value_mask],\n",
    "                return_attention_scores = True,\n",
    "            )\n",
    "            shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "            shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "            return context_vector, attention_weights "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
